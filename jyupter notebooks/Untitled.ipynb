{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "import pandas as pd\n",
    "from minisom import MiniSom  \n",
    "import math\n",
    "import ipynb\n",
    "from sklearn.metrics import classification_report\n",
    "import cv2\n",
    "from itertools import chain\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions from seperate workbook\n",
    "from ipynb.fs.full.helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_som(x_train, n_features):\n",
    "    \n",
    "    # Create SOM dimensions\n",
    "    som_nurons = int((math.sqrt(5*math.sqrt(n_features))))\n",
    "#     print(som_nurons)\n",
    "    x = som_nurons\n",
    "    y = som_nurons\n",
    "    \n",
    "    #Create and train SOM\n",
    "    som = MiniSom(x, y, n_features, sigma=0.3, learning_rate=0.5) # initialization of 6x6 SOM\n",
    "    som.random_weights_init(x_train)\n",
    "    print(\"Training...\")\n",
    "    som.train_random(x_train,100, verbose=False) # training with 100 iterations\n",
    "    print(\"...ready!\")\n",
    "    return som"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of images to train the SOM\n",
    "image_list_train = x_train[0:1000]\n",
    "image_list_test = x_test[0:1000]\n",
    "image_list_res_test = y_test[0:1000]\n",
    "image_list_res_train = y_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have to flatten our data so we are able to feed it to the SOM\n",
    "x_test_r = flatten_and_reshape(image_list_test)\n",
    "x_train_r = flatten_and_reshape(image_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "...ready!\n"
     ]
    }
   ],
   "source": [
    "som = create_train_som(x_train_r, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_map = som.distance_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32879689, 0.51005443, 0.49372258, 0.56741816, 0.50097544,\n",
       "        0.51456093, 0.53939002, 0.45505484, 0.45936083, 0.52610166,\n",
       "        0.33945012],\n",
       "       [0.53739866, 0.84269825, 0.89601312, 0.7671505 , 0.92222467,\n",
       "        0.79617196, 0.73256427, 0.75308446, 0.77132337, 0.81845596,\n",
       "        0.57490193],\n",
       "       [0.55170266, 1.        , 0.71186579, 0.87742692, 0.85157369,\n",
       "        0.70120936, 0.64521037, 0.82187442, 0.88642535, 0.89196143,\n",
       "        0.56505198],\n",
       "       [0.51319404, 0.74866177, 0.78180027, 0.95352028, 0.90362797,\n",
       "        0.79866196, 0.78054678, 0.75524868, 0.91222536, 0.92016344,\n",
       "        0.53483491],\n",
       "       [0.47778643, 0.89376811, 0.77388448, 0.80200033, 0.91571686,\n",
       "        0.87424071, 0.716983  , 0.80547858, 0.84452609, 0.75381258,\n",
       "        0.5581976 ],\n",
       "       [0.46219661, 0.66367452, 0.77283491, 0.88155411, 0.78131788,\n",
       "        0.85202379, 0.83411983, 0.78781499, 0.80797132, 0.87296267,\n",
       "        0.53106695],\n",
       "       [0.46058439, 0.70049786, 0.87100637, 0.82433364, 0.8356733 ,\n",
       "        0.94825695, 0.86579449, 0.85250865, 0.78622274, 0.91005841,\n",
       "        0.49084293],\n",
       "       [0.45281946, 0.69127272, 0.7714451 , 0.83360242, 0.84969659,\n",
       "        0.87625175, 0.84407339, 0.89363991, 0.84831956, 0.83256578,\n",
       "        0.47058518],\n",
       "       [0.51134062, 0.82103434, 0.72245001, 0.7559383 , 0.96975221,\n",
       "        0.75157317, 0.8920113 , 0.81003076, 0.73907697, 0.83255683,\n",
       "        0.45147794],\n",
       "       [0.58611433, 0.79783675, 0.88678523, 0.79850623, 0.77802457,\n",
       "        0.82410376, 0.73651057, 0.76114113, 0.77160402, 0.7041176 ,\n",
       "        0.55889899],\n",
       "       [0.33088906, 0.57107657, 0.52620906, 0.57584769, 0.45810674,\n",
       "        0.52090851, 0.51760475, 0.47491002, 0.51788136, 0.53576815,\n",
       "        0.27467613]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-17-11e1d07b02e2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-11e1d07b02e2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    min(np.array(distance_map.flat)\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "min(np.array(distance_map.flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\_distance_from_weights function allows us to extract the distance map for the SOM and the given training example. The distance map contains the distance from each node within the SOM to the given training example. Using this we can extract the winning nodes distance to use in our convolutional layer.\n",
    "\n",
    "So how do we creat the secondary layer? \n",
    "\n",
    "Ok we could do what the boss says, use the coordinants and the weight of the winning neuron for that example. \n",
    "\n",
    "what else / how would we index that. I mean we could just do exactly what we did last time index on observation number. \n",
    "\n",
    "What do we call this intermediate layer? whereby we combine two SOMs - convolutional layer? Well yeah beacuse were combining two seperate things. So maybe we need to work on thhat\n",
    "\n",
    "OK step 1 - lets just create a very basic workflow using incredibly manual tasks. I mean lets just do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = som._distance_from_weights(x_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1225.94669461, 1010.70187452,  711.45555026, ...,  997.6807901 ,\n",
       "        1069.95438037,           nan],\n",
       "       [          nan,           nan,           nan, ...,  527.01255892,\n",
       "                  nan,           nan],\n",
       "       [1973.27287486, 1663.7671456 , 1122.11719531, ..., 1520.12787533,\n",
       "        2250.47663561, 1382.98588575],\n",
       "       ...,\n",
       "       [ 755.65522101,           nan,           nan, ..., 1266.66252103,\n",
       "                  nan,           nan],\n",
       "       [1055.25740128, 1163.65077494, 1187.42326068, ...,  941.188829  ,\n",
       "        1184.25491051,  937.55397279],\n",
       "       [1670.75613478,           nan,           nan, ...,  853.56621311,\n",
       "        1751.24426507,           nan]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
